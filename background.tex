\chapter{Background}
\label{ch:background}
\todo{WF comment about equational FOL definition: the transitivity rule is not needed and the last rule is universal instantiation} 
\todo{conservative extensions: why would they act better under little theories than under big theories?}
In this chapter, we discuss some background concepts needed for presenting our work. 
Our ideas and implementation are based on dependent type theory (DTT). It is the meta theory for this work. We introduce it in Section~\ref{subsec:background:dtt} and define the notion of a theory and a context in Section~\ref{sec:background:theory}.
% Equational logic is based in set theory and forms the bases for universal algbera.  Dependent type theory (DTT) is a rich type theory in which types can depend on values. We want to represent theories of equational logic as contexts in DTT. We first introduce equational logic and DTT in Section~\ref{sec:background:logic}. 
%Contexts are defined in Section~\ref{sec:background:context} and theories in Section~\ref{sec:background:theory}. We then briefly introduce universal algebra and how it is used in our work in Section~\ref{sec:background:ualgebra}. 

Part of our work is building a library of axiomatic theories. The library is organized as a theory graph, in which theories are connected via morphisms. We introduce morphisms in Section~\ref{sec:background:morphisms} and discuss theory graphs and different strategies to building them in Section~\ref{sec:background:theorygraph}. 
To build the library we use combinators motivated by category theory, so we give a brief introduction for it in Section~\ref{sec:categoryTh}. 
One of the constructions we generate, that is not typically defined within universal algebra texts is multi-staged programming, so we define it in Section~\ref{sec:background:msp}. 

\section{Dependent Type Theory}
\label{subsec:background:dtt}
Dependent type theory (DTT) is a version of type theory that enables writing types like \lstmath{$\lambda\ $ x : A $\ \cdot\ $ M} where the type \lstmath{M} depends on the \emph{value} of \lstmath{x}. 
% source: Type theory and formal proof, P. 103 - Ch.5 
Having types that depends on values adds to the expressiveness of the logic. A common example for introducing dependent types is the type of a vector of \lstmath{n} elements of type \lstmath{A}. 
In most programming languages, the type of this vector is defined in terms of the type of its elements as \lstmath{Vec A}. Using dependent types, the type of a vector can depend on both the type of its elements and also its length, \lstmath{$\lambda\;$ n : Nat $\;\cdot\;$ Vec A n}, also written as \lstmath{$\Pi\;$ n : Nat $\;\cdot\;$ Vec A n}.
 
Having this extra piece of information in the type allow detection of some errors, like accessing out of bounds elements, on the type checker level. Type checking, then, guarantees that this kind of error does not exist in the given implementation. 
DTT has gained popularity and is used as 
Powerful type systems like DTT paves the way to building correct-by-construction software where the type of a function is its specification and type checkers are used to verify that the implementation follows the specification. 

%,  the specification is the type of the function 
%The descriptive power of DTT makes it more suitable for capturing mathematical knowledge. Therefore, many of the modern mechanized mathematics systems use variants of DTT as their foundations.\ednote{check this}  

\paragraph{$\Sigma$-types.}
A dependent pair, in which later elements depends on values of earlier ones, are referred to as $\Sigma$-types. The type \lstmath{$\Sigma\;$ (n : Nat, Vec A n)} refer to a pair that contains the value of \lstmath{n} in the first position and a vector of length \lstmath{n} in the second one. 

\paragraph{Telescopes.}
The concept of $\Sigma$ types is generalized into that of \emph{telescopes}, or equivalently,  \lstmath{dependent-type records}~\cite{pollack2002dependently}. A telescope $\mathbb{T}$ is defined as 
\begin{equation}
\mathbb{T} \equiv [x_1 : A_1][x_2 : A_2(x_1)][x_k : A_k(x_1,\cdots,x_{k-1})]
\label{eq:telescope}
\end{equation} 
The type \lstmath{Vec A n} as a telescope would be written as 
\lstmath{[A : Set][n : Nat][Vec A n]}. 
%Another related concept is the packaging of definitions in a structure in which a definition depends on earlier ones. This is called a dependent record type, and is referred to as $\Sigma$-types. The types \lstmath{A} and \lstmath{P:A $\;\to\;$ Type} can be grouped is a dependent pair $\Sigma A P$. 

%\section{Curry Howard Correspondence}
%Also known as propositions-as-types and proofs-as-programs, the Curry-Howard correspondence relates constructive logics, like dependent type theory, to computer programs.\ednote{expand this}.  

\paragraph{Contexts.}
%\label{sec:background:context}
In logic, A proposition is true if it is an axiom or is derivable from other true propositions using inference rules. This is usually written as $\varphi_1 ... \varphi_n \vdash \psi$.  In categorical logic, instead of talking about propositions, one talk about contexts. \cite{handbook1993CategoricalLogic} defines a context as 
\begin{quote}
A context, $\Gamma$, is a finite list $[x_1 : \sigma_1, ... , x_n : \sigma_n]$ of (variable,sort) pairs, subject to the condition that $x_1, ... , x_n$ is distinct.  
\end{quote}
When using dependent types, the context becomes a telescope, where every type in the list can contain reference to variables before it as described by Equation~\ref{eq:telescope}. The statement $\Gamma \vdash \psi$ means that the type judgement $\psi$ follows from the context $\Gamma$. The concatenation of two contexts $\Gamma_1$ and $\Gamma_2$ is noted by $\Gamma_1 ; \Gamma_2$.  
%\paragraph{The category of contexts}

\section{Theories}
\label{sec:background:theory}
%The set of formulas is called the language of the logic. The language is defined syntactically; there is no notion of meaning or semantics in a  logic per se. 
A theory  in some logic is defined as the tuple \lstmath{($\sort$,$\fsyms$,$\axioms$)} such that 
\begin{itemize}
\item $\sort$ is a set of sorts 
\item $\fsyms$ is a set of function symbols. 
\item $\axioms$ is the set of formulas that holds in $\Gamma$. 
\end{itemize}
The sort in $\sort$ and the function symbols in $\fsyms$ constitute the language of the theory. 
The set $\axioms$ is closed under logical consequence and usually infinite. 
A \emph{theory presentation} of some theory $\Gamma$ includes a finite set of sorts, a finite set of function symbols, and a finite set of generating axioms, i.e. axioms from which additional formulas can be derived by inference. 
Note that the same theory can have different theory presentations. 
In this work, as is traditionally the case, we use the term theory to refer to theory presentations. 
%Most algebra text books would refer to the language of the theory presentation as the signature and would separate its two components, writing it as \lstmath{(S,F)}. They often do not use the term \lstmath{theory}, but refer to sigantures with specific properties that are satisfied by the algebras. The use of axiomatic theories as we define them here is, to the best of our knowledge, tied to using them in computerized systems as in Clear~\cite{Goguen1980}. 

\paragraph{Theories as Contexts}
With dependent types and the Curry-Howard correspondance in place, the distinction between the three components of an axiomatic theory, sorts, function symbols, and axioms, is not needed anymore. Instead a theory is seen as a $\Sigma$ type, dependently-typed context, or a telescope as described by Equation~\ref{eq:telescope}. 
For example, the axiomatic formalization of \lstmath{Monoid} as a $\Sigma$ type is 
\begin{lstlisting}[mathescape]
$\Sigma$ (A : Set, $\Sigma$ (op : A $\;\to\;$ A $\;\to\;$ A, $\Sigma$ (e : A, 
  $\Sigma$ (lunit : {x : A} $\;\to\;$ op e x = x,
    $\Sigma$ (runit : {x : A} $\;\to\;$ op x e = x,
      $\Sigma$ (assoc : {x y z : A} $\;\to\;$ op x (op y z) = 
                                op (op x y) z))))))
\end{lstlisting} 
which can be describe as a telescope as  
\begin{togcode} 
Monoid = [A : Set, op : A ~$\;\to\;$~ A ~$\;\to\;$~ A, e : A, 
          lunit : {x : A} ~$\;\to\;$~ op e x = x, 
          runit : {x : A} ~$\;\to\;$~ op x e = x, 
          assoc : {x y z : A} ~$\;\to\;$~ op x (op y z) = op (op x y) z] 
\end{togcode} 
This definition induces a context from which the type \lstmath{op e e = e} can be defined, which is noted as \lstmath{Monoid $\ \vdash\ $ triv : op e e = e}  

%The telescopic representation of a sort would be \lstmath{$\sort$:$\square$} where $\square$ is a kind defined in the type theory. A function symbol within a telescope is defined in terms of the type of its parameters. Propositions-as-types make it possible to present axioms as part of telescopes. is declared as $\sort : Type$. 
%Every declaration \lstmath{c:t} within the telescope is defined within a context \lstmath{$\Gamma$}. This is described as \lstmath{$\Gamma \vdash\;$ c:t}. 

%Theories in DTT are seen as contexts. This is how they are presented in~\cite{carette2018building} which forms the basis of this work. Therefore, we use the term context to also mean a theory in DTT. 

A theory presentation is well-typed if every declaration \lstmath{c:t} is well-typed given its context. The formation rules for theory presentations are given in Figure~\ref{fig:ctx}, where $\syms{\Gamma}$ refers to the list of symbols defined in the context $\Gamma$. 
$$ \syms{\context{\varnothing}} = \EmptyThy \qquad
\syms{\context{\Gamma}\ ;\ x : \sigma} = \syms{\context{\Gamma}} \cup \left\{ x \right\}
$$
\begin{figure}[ht]
    \begin{proofrules}
        \[ \ \justifies \varnothing \ \wfctx \]
        \[ \context{\Gamma}\ \wfctx \qquad \sigma \notin \syms{\context{\Gamma}}
        \qquad \context{\Gamma} \vdash \kappa : \Box \justifies
        (\context{\Gamma}\ ;\ \sigma : \kappa)\ \wfctx \]
        \[ \context{\Gamma}\ \wfctx \qquad x\notin \syms{\context{\Gamma}}
        \qquad \context{\Gamma} \vdash \sigma : \kappa : \Box \justifies
        (\context{\Gamma}\ ;\ x : \sigma)\ \wfctx \]
    \end{proofrules}
    \caption{Formation rules for contexts as introduced in~\cite{carette2018building}}
    \label{fig:ctx}
\end{figure}

\section{Theory Morphisms}
\label{sec:background:morphisms}
Morphisms are used to capture the structure of mathematics, by describing how theories are related to each other. In mathematical texts, a theorem proved for an arbitrary \lstmath{Monoid} can be used when considering an arbitrary \lstmath{Group} without extra work. Formally, this can be done if a meaning preserving morphism between \lstmath{Monoid} and \lstmath{Group} exists. The morphism specifies how results in \lstmath{Monoid} can be interpreted in \lstmath{Group}. 

A morphism $\arrow{[v]}{\Gamma}{\Delta}$ consist of a list of assignments $[v]$, a source theory \lstmath{$\Gamma$}, and a target theory \lstmath{$\Delta$}. $[v]$ assigns to every symbol in $\Gamma$ an expression of $\Delta$. A term \lstmath{t} in the language of $\Gamma$ can be translated into a term \lstmath{t$^\prime$} in the language of $\Delta$ using substitution, such that  \lstmath{t$^\prime$ = t$[v]$}. Using the morphism 
\lstmath{[op to + ; e to 0] : Monoid $\;\to\;$ AdditiveMonoid}
we are able to interpret the expression \lstmath{(op e x)} in \lstmath{Monoid} as \lstmath{(+ 0 x)} in \lstmath{AdditiveMonoid} using substitution. 
%For example, a morphism from \lstmath{Monoid} to \lstmath{AdditiveMonoid} contains the assignments \lstmath{op to +} and \lstmath{e to 0} 

The formation rules for views, as given in~\cite{carette2018building} is given in Figure~\ref{fig:views}. 
\begin{figure}[ht]
    \begin{proofrules}
        \[ \context{\Delta}\ \wfctx \justifies \view{}{\varnothing}{\Delta} \]
        \[ (\context{\Gamma}\ ;\ x : \sigma)\ \wfctx \qquad
        \view{v}{\Gamma}{\Delta} \qquad
        \context{\Delta} \vdash r : \substitution{\sigma}{v}{} \justifies
        \view{v,x \mapsto r}{(\context{\Gamma}\ ;\ x : \sigma)}{\context{\Delta}} \]
    \end{proofrules}
    \caption{Formation rules for morphisms.}
    \label{fig:views}
\end{figure}

It is worth mentioning that the mapping is only a part of the morphism. A morphism consists of the source and destination theories as well as the mapping, i.e. the same substitution can induce different morphisms as the source and target are modified. 

Connecting theories have been known for a long time in logic~\cite{tarski1953undecidable, enderton1972mathematical} under the name \emph{theory interpretations}. The same name is used by IMPS~\cite{farmer1993imps, InterpIMPS1994}. Clear~\cite{Goguen1980}, OBJ and their successors used the term \emph{morphisms}, maybe because of using category theory for semantics. The term \emph{view} has also been used to refer to the same concept by Maude, MathScheme, and MMT. In this work, we use the terms views and morphisms interchangeably. 

We distinguish between three type of morphisms 

\subsection{Identity Morphism}
\label{sec:idmorph}
If $\arrow{[v]}{\Gamma}{\Delta}$ is an identity morphism, then $[v]$ maps every symbol $x \in \syms{\Gamma}$ to itself such that $x[v] = x$.
%This implies that $[v]$ is a bijection and that . 
While it is common to name source and target of identities with the same name, we do not do that here as $\Gamma$ and $\Delta$ are two different theory presentations. The identity between them means that symbols in $\Gamma$ are interpreted the same way in $\Delta$. 

Identity morphisms exist between two theories if the source is included verbatim in the destination, like in the case when \lstmath{Group} is defined as an extension of \lstmath{Monoid}. It is the simplest form of morphisms and allow transport of results without the need to perform substitution 

\subsection{Embedding}
\label{sec:embedding}
If $\arrow{[v]}{\Gamma}{\Delta}$ is an embedding, then $[v]$ maps every symbol $x \in \syms{\Gamma}$ to a symbol $r \in \syms{\Delta}$, which is not necessarily itself. 

Consider for example, the following morphism from \verb|Magma| to \verb|AdditiveMagma|
\begin{equation*}\label{eq:additiveview}
\begin{tikzpicture}[node distance=9.0cm, auto,baseline=(current bounding box.center)]
\node (P) {$
    \begin{thyex}
    \thyrow{A}{\tmop{Type}}
    \thyrow{op}{A \rightarrow A \rightarrow A}
%    \thyrow{assoc_op}{\cdots}
    \end{thyex} $};
\node (B) [right of=P] {$
    \begin{thyex}
    \thyrow{A}{\tmop{Type}}
    \thyrow{+}{A \rightarrow A \rightarrow A}
%    \thyrow{assoc_+}{\cdots}
    \end{thyex} $};
\draw[->] (P) to node {$[A \mapsto\ A, 
    op \mapsto\ + ]$} (B);
\end{tikzpicture}
\end{equation*}
A term $t \in \Gamma$ is transported to $\Delta$ as $t[v]$, i.e.: by applying the substitution $[v]$ to the term $t$. 
So if \lstmath{$t =\;$ op x y}, then using the morphism above it is transported to $\Delta$ as \lstmath{(+ x y)}. 

We refer to an embedding morphism as $\tilde{m}$, and therefore identity morphisms are referred to as $\tilde{id}$. 

\subsection{General Morphism}
\label{sec:generalmorph}
A morphism $\arrow{[v]}{\Gamma}{\Delta}$ is a general morphism if it maps symbols $x \in \syms{\Gamma}$ to terms $r \in \Delta$. 

An example is a morphism that flips a binary operation, i.e.: maps \lstmath{op x y} to \lstmath{op y x}
\begin{equation}\label{eq:flipmagmaview}
\begin{tikzpicture}[node distance=9.0cm, auto,baseline=(current bounding box.center)]
\node (P) {$
    \begin{thyex}
    \thyrow{A}{\tmop{Type}}
    \thyrow{op}{A \rightarrow A \rightarrow A}
    \end{thyex} $};
\node (B) [right of=P] {$
    \begin{thyex}
    \thyrow{A}{\tmop{Type}}
    \thyrow{op}{A \rightarrow A \rightarrow A}
    \end{thyex} $};
\draw[->] (P) to node {$[A \mapsto\ A,
    op \mapsto\ \mathsf{flip}\ op]$} (B);
\end{tikzpicture}
\end{equation}

%\section{Substitution}
%Morphisms acts on theories by substitution, i.e.: Given a morphism $[v] : \Gamma \to \Delta$, by performing the substitution of mappings in $[v]$ to symbols in $\Gamma$, we get the presentation $\Delta$. This substitution is written as $\Gamma[v]$. 
%Mappings acts on theories by substitution, i.e.: Given a list of assignments of terms to symbols $[x \mapsto y]$ and an expression e, a substitution replaces every free occurence of $x$ in the theory by y. 

\section{Theory Graph}\label{sec:background:theorygraph}
One way to organize theories is using theory graphs. A theory graph is a directed acyclic graph consisting of theories as nodes and morphisms as edges between them. It is helpful in managing large libraries~\cite{kohlhase2010towards}.

In systems that are based on categorical semantics, a theory graph is seen as a diagram in the category of theories and theory morphisms. Specware~\cite{Smith99} uses the keyword \emph{diagram} to build them. The work in~\cite{developmentGraph2000}, based on CASL, refer to them as \emph{development graphs}. 

Organizing a library as a theory graph leverages the structure of mathematics by relying on morphisms to connect the different concepts presented within the theories. Compare a library defining the graph leading to  \lstmath{Monoid} as in Figure~\ref{fig:cube_monoid} to one that defines it only in terms of its components, as in Section~\ref{sec:background:theory}. The theory graph one has more information which makes it more useful to library users. 
Theory graphs also makes it possible to modularize a formalization by adding definitions or proving theorems within smaller modules (theories). Definitions and theorems are then made available to different other theories by transporting them via morphisms. 

Here we discuss two strategies for decomposing theories; little and tiny theories.  

%By comparing the theory graph in Figure~\ref{fig:cube_monoid} with a definition of \lstmath{Monoid} in terms of its declarations shows how using a graph to define \lstmath{Monoid} leverages i
  
%Theory graphs leverage the structure of mathematics by relying on morphisms to connect the different concepts presented within the theories. 
%This way it is possible to add definitions and proof theorems within one theory 
%Therefore, it becomes possible to decompose definitions and proofs to pieces of knowledge into smaller contexts and perform reasoning within these contexts. In cases when this knowledge is needed in a different context, then transport them to different ones when needed. we are able to decompose the development of theories. Having morphisms also facilitates the transportation of results between theories. 

\subsection{Little Theories}
The little theories approach is introduced in~\cite{LittleTheories}. The idea is to ensure that if a statement \lstmath{s} is proven in context \lstmath{$\Gamma$}, then every statement in \lstmath{$\Gamma$} is required to prove \lstmath{s}. In this case, we say \lstmath{$\Gamma$} is the \emph{minimal axiomatization} needed to prove \lstmath{s}. This implies that theorems are proved in different contexts based on the amount of structure needed to prove them. In contrast, the big theory approach would a small set of big theories for proving all results\footnote{Or a medium-sized set of medium-sized theories}. 

Using little theories increases the reuse of results. For example, if the theorem \lstmath{op e e = e} is proven in the theory \lstmath{Unital}, it can be transported to all theories that are connected to \lstmath{Unital} via morphisms, like \lstmath{Monoid}. On the other hand, if it is proven in the theory \lstmath{Group}, it cannot be transported to \lstmath{Monoid}, because all declarations in \lstmath{Group} becomes part of the context for proving the theorem.  
%to a less number of theories under the assumption that \lstmath{Group} is defined from \lstmath{Unital} using a series of extensions. 
%Results proven in the theory \lstmath{$\Gamma$} can be transported to all theories \lstmath{$\Delta$} whenever a morphism $m : \Gamma \to \Delta$ exists. 
% increasing usability and reducing redundancy when dealing with formal systems.  

\subsection{Tiny Theories}
\label{sec:background:tinytheories}
Tiny theories is a refinement of little theories, in which only one new piece of information is added at a time~\cite{mathscheme2011experiments}. To make this more clear, let's consider a library that has the theories \lstmath{PointedMagma} and \lstmath{Unital} defined as follows. \\
\begin{tabular}{p{7cm} p{7cm}}
\begin{lstlisting}[mathescape, basicstyle=\footnotesize]
theory PointedMagma = { 
  A : Type 
  e : A 
  op : A $\to$ A $\to$ A }
\end{lstlisting}
&
\begin{lstlisting}[mathescape, basicstyle=\footnotesize]
theory Unital = {
  A : Type 
  e : A 
  op : A $\to$ A $\to$ A 
  lunit : {x : A} $\to$ op e x = x
  runit : {x : A} $\to$ op x e = x   }   
\end{lstlisting}
\end{tabular}
Defining \lstmath{Unital} this way overlooks that in some cases one might want to define a theory to describe structures with a carrier and a binary operation on it that has only a right unit, like a theory with Integers as carrier and subtraction as the only binary operation. 
One will then need to add a new theory that is similar to \lstmath{Unital} without the \lstmath{lunit} declaration. Any theorems proved in the context of \lstmath{Unital} cannot be used, even if it only depends on \lstmath{runit}. 

Using tiny theories, one would first define a \lstmath{LeftUnital} theory adding the \lstmath{lunit} axiom to \lstmath{PointedMagma}, a \lstmath{RightUnital} theory adding \lstmath{runit} axiom, and the theory of \lstmath{Unital} would be connected to both \lstmath{LeftUnital} and \lstmath{RightUnital}, creating more connections and therefore, allowing more reuse of results. Systematically using tiny theories to develop a large library leads to the need for support to diamond structures, which we discuss in Chapter~\ref{ch:library} based on the work in \cite{carette2018building}.  

\section[Category Theory]{Category Theory\footnote{This section is based on~\cite{pierce1990taste}}}
\label{sec:categoryTh}
Category theory is a foundational framework, like set and type theory, that is abstract and structured enough to allow hidden commonalities of concepts to emerge. 

While set theory has elements of sets as the main concept, category theory is built around the concept of morphisms. The source and target of a morphism are objects in the category. Category theory is not concerned with the internal structure of the objects, but rather by how they relate to other objects. 

A category $\mathcal{C}$ consists of 
\begin{itemize}
\item A collection of objects, $\syms{\mathcal{C}}$
\item A collection of morphisms between the objects. A morphism is represented as $u : \Gamma \to \Delta$.  
\item Operations assigning to every morphism to its domain and codomain 
\item A composition function $\cdot$ assigning to each pair of morphisms  $u : \Gamma \to \Delta$ and $v : \Delta \to \Phi$, a morphism $u \cdot v : \Gamma \to \Phi$, such that 
\[ u \cdot (v \cdot w) = (u \cdot v) \cdot w \]
\item For every object $\Gamma$ in $\mathcal{C}$, an identity morphism $id_\Gamma : \Gamma \to \Gamma$, such that for $u : \Gamma \to \Delta$ 
\[ id_\Gamma \cdot u = u \cdot id_\Delta = u \]
\end{itemize}

Finite categories can be represented diagrammatically as in Figure~\ref{fig:diagram}. 
\begin{figure}
\centering{
\begin{tikzcd}
 & \Gamma \arrow[loop] \arrow[dr]& \\
\Delta \arrow[loop left] \arrow[rr] \arrow[ur] & & \Phi \arrow[loop right] 
\end{tikzcd}}
\caption{A diagram in a category with at least $3$ objects}
\label{fig:diagram}
\end{figure}
Such a diagram is said to commute if for every pair of vertices, $\Gamma$ and $\Delta$, all paths from $\Gamma$ to $\Delta$ are equal. 

In the following we introduce two concepts related to categories that we use in Chapter~\ref{ch:library}. These are pushouts and colimits. \cite{nlab:colimit} gives an intuition of what a colimit is as 
\begin{quote}
``The intuitive general idea of a colimit is that it defines an object obtained by sewing together the objects of the diagram, according to the instructions given by the morphisms of the diagram"
\end{quote}
A pushout is a special case of a colimit. In~\cite{nlab:pushout}, it is mentioned that 
\begin{quote}
``A pushout is the colimit of the diagram 
\begin{tikzcd} 
\bullet & \bullet \arrow[l] \arrow[r] & \bullet
\end{tikzcd}"
\end{quote}
The formal definitions of the two constructions are
\paragraph{Pushouts.}
The pushout of a pair of arrows $u_\Delta : \Gamma \to \Delta$ and $u_\Phi : \Gamma \to \Phi$ is an object $\Xi$ and a pair of arrows $v_\Delta : \Delta \to \Xi$ and $v_\Phi : \Phi \to \Xi$ such that 
\begin{itemize}
\item $u_\Delta \cdot v_\Delta = u_\Phi \cdot v_\Phi $
\item for morphisms $w_\Delta : \Delta \to \Omega$ and $w_\Phi : \Phi \to \Omega$, there is a unique $w : \Xi \to \Omega$, such that 
$v_\Delta \cdot w = w_\Delta$ and $v_\Phi \cdot w = w_\Phi$
\end{itemize}
The definition of the pushout is illustrated in Figure\ref{fig:pushoutDef}. 
\begin{figure}
\centering{
\begin{tikzcd} 
\Gamma \arrow["u_\Delta",d] \arrow["u_\Phi",rr]& & \Phi \arrow["v_\Phi",d] \arrow["w_\Phi",rdd, bend left] \\
\Delta \arrow["v_\Delta",rr] \arrow["w_\Delta",rrrd, bend right]& & \Xi \arrow[dashed, rd] \\ 
& & & \Omega
\end{tikzcd}} 
\caption{Diagram illustrating the definition of a pushout}
\label{fig:pushoutDef}
\end{figure}

\paragraph{Colimits.}
Colimits are defined in terms of cocones. The definitions we present here are adapted from~\cite{sannella2012foundations}. 

A cocone over a diagram consists of an object $\Gamma$ and a family of morphisms $u_0 : \Delta_0 \to \Gamma, ..., u_n : \Delta_n \to \Gamma$, where $n$ is the number of nodes in the diagram other than $\Gamma$, such that for every morphism $v : \Delta_i \to \Delta_j$ in the diagram $v \cdot u_j = u_i$, i.e. the following diagram commutes 
\begin{tikzcd} 
& \Gamma & \\ 
\Delta_i \arrow[ur,"u_i"] \arrow[rr,"v"] & & \Delta_j \arrow[ul,"u_j"] 
\end{tikzcd}
The notation used to descrine cocones is 
$\langle u_i : \Gamma \to \Delta_i \rangle_{i\leq n}$.

The colimit of a diagram is a cocone  
$\langle u_i : \Gamma \to \Delta_i \rangle_{i\leq n}$ such that for any cocone 
$\langle u_i^{\prime} : \Gamma^{\prime} \to \Delta_i \rangle_{i\leq n}$ there is a unique morphism $v : \Gamma \to \Gamma^\prime$ such that for every $u_i$, the following diagram commutes 
\begin{tikzcd} 
\Gamma \arrow[ rr,dashed, "v"] && \Gamma^\prime \\ 
& \Delta_i \arrow[ul,"u_i"] \arrow [ur," u_i^{\prime}"] & 
\end{tikzcd} 

\section{Multi-Staged Programming}
\label{sec:background:msp}
Meta-programming is the practice of writing \emph{meta} programs that manipulate \emph{object} programs. Meta and object programs can be in the same or different languages. Generative programming is one form of meta-programming in which the meta-program compiles into a program of the object language. Therefore, the process of running the meta-program involves two stages, compile and run-time. 

The meta program might need to refer to code in the object language, like in the case of making a call to a predefined function in the object language. In this case, the meta program is deferring the evaluation of this code to a \lstmath{later} stage. 
Also, a meta program might need to evaluate a meta or object language expression that results in an object code. In this case, the expression is evaluated in the \lstmath{current} stage. 

In our implementation, we define two stages \lstmath{s0} and \lstmath{s1}. 
\begin{togcode}
data Stage : Set where
  s0 : Stage
  s1 : Stage
\end{togcode} 

Staging an expression means adding annotation to its components indicating which stage it should be evaluated in. 
\lstmath{Now} or \lstmath{Later}. 
\begin{togcode} 
data Staged (A : Set) : Set where
  Now : A -> Staged A
  Later : Comp A s1 -> Staged A
\end{togcode} 
Annotating an expression of type \lstmath{A} with the \lstmath{Now} constructor indicates that it will be evaluated in the current stage and a value of type \lstmath{A} is promised to exist. On the other hand, if the evaluation is deferred to \lstmath{Later}, then the expression will have the type \lstmath{Comp}, for computation. 
\begin{togcode} 
data Comp (A : Set) (s : Stage) : Set where
  Computation : Choice -> CodeRep A s -> Comp A s
\end{togcode} 
Computations encapsulate quoted fragments of code. The \lstmath{CodeRep} function assigns a stage \lstmath{s0} or \lstmath{s1} to the expression. 
\begin{togcode} 
data Wrap (A : Set) : Set where
  Q : A -> Wrap A
CodeRep : (A : Set) (s : Stage) -> Set
  CodeRep A s0 = A
  CodeRep A s1 = Wrap (CodeRep A s0)
\end{togcode} 
We also add a flag indicating whether the quoted code represent an expression (\lstmath{Expr}) or a literal, constant or variable (\lstmath{Const}).  
\begin{togcode} 
data Choice : Set where
  Expr : Choice
  Const : Choice
\end{togcode} 

There are $3$ main applications to staging; generating well-typed code as in MetaOcaml~\cite{taha1999multi}, removing abstraction overhead introduced by generic programming~\cite{yallop2016StagingGeneric, carette2011mspFunctorsMonads, carette2011generative}, and developing domain specific languages~\cite{sheard2000stagingDSL}. MetaOcaml and Haskell templates provide staging constructs under the names \emph{quote} and \emph{eval} instead of \emph{Now} and \emph{Later}. In logical reasoning the same ideas are used for reflection, as in~\cite{farmer2013quoteEval}.  


%MSP gives the developers of the meta program the control over it by annotating which pieces can be evaluated at runtime\ednote{better writing}.  
%Writing program generators typically involve multiple stages; generation, compilation, and runtime stage. 
%MSP is a technique for managing code generation across different stages until execution. to provide annotations for the generator 

%\section{Finally Tagless}
%\label{sec:background:tagless}
%While staging is useful in so many ways, it introduces the overhead of dealing with the tags to \lstmath{Now} and \lstmath{Later}, or \lstmath{quote} and \lstmath{eval}. The finally tagless approach aim to remove this tagging overhead by encoding the object language as functions within a class, instead of constructors of a datatype. For example, the language of monoid would be represented as a type by: 
